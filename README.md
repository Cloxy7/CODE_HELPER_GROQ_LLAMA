# Code Q\&A Bot using Groq LLaMA and FAISS

## ğŸ“Œ Project Overview

This project builds a **Code Q\&A Assistant** that can intelligently answer programming-related questions by retrieving relevant code snippets and generating explanations or suggestions. It combines semantic search using **FAISS** and **Sentence Transformers**, and natural language generation using **Groq's LLaMA-3 model**.

---

## ğŸ”§ Step-by-Step Breakdown

### 1. ğŸ”— Install Dependencies

```python
!pip install -q sentence-transformers faiss-cpu requests
```

**Why:**

* `sentence-transformers` is used for embedding code snippets and queries.
* `faiss-cpu` is used for fast similarity search.
* `requests` is used to call the Groq API.

---

### 2. ğŸ” Set API Key

```python
GROQ_API_KEY = "<your_api_key>"
```

**Why:**

* Groq's API key is needed to authenticate and access LLaMA-3 models.

---

### 3. ğŸ§© Define Code Snippets

```python
code_chunks = [
    "def sort_list(lst): return sorted(lst)",
    "def reverse_string(s): return s[::-1]",
    "def factorial(n): return 1 if n == 0 else n * factorial(n - 1)",
    "def is_prime(n): return all(n % i != 0 for i in range(2, int(n ** 0.5) + 1)) and n > 1"
]
```

**Why:**

* These are sample code snippets we want to retrieve from when a user asks a question.

*Note:* You can scale this list as needed or replace it with dynamic code ingestion.

---

### 4. ğŸ“ˆ Embed & Index Code Snippets

```python
embed_model = SentenceTransformer('all-MiniLM-L6-v2')
chunk_embeddings = embed_model.encode(code_chunks, convert_to_numpy=True)

index = faiss.IndexFlatL2(chunk_embeddings.shape[1])
index.add(chunk_embeddings)
```

**Why:**

* Converts code snippets to dense vectors using MiniLM embeddings.
* Adds them to FAISS index for similarity search.

---

### 5. ğŸ§  Define LLM Call Function

```python
def call_groq_llm(prompt, api_key=GROQ_API_KEY):
    # (API call using Groq LLaMA-3)
```

**Why:**

* Sends prompt to Groq and gets the answer from `llama3-8b-8192`.

---

### 6. ğŸ§¾ Define Q\&A Function

```python
def ask_code_bot(user_query, top_k=2):
    # Embeds user query
    # Retrieves top-k code chunks
    # Builds prompt and returns answer from Groq LLM
```

**Why:**

* Core logic that ties together retrieval and generation to answer user queries.

---

### 7. ğŸ“Š Evaluation Script

* Evaluates on 20 manually labeled queries
* Captures:

  * Ground truth index
  * Predicted index
  * Latency
  * Helpfulness score (assumed static)

```python
f1_score, precision_score, recall_score, latency = ...
```

**Why:**

* Provides objective metrics for model performance.

---

## ğŸ“Š Final Evaluation Results

```
âœ… Top-2 Retrieval Accuracy: 0.95
ğŸŒŸ Average Helpfulness Score: 0.90
âš¡ Average Latency: 0.6963 seconds
ğŸ¯ F1-Score: 0.95
```

---

## ğŸ’¡ Conclusion

We built a robust, low-latency Code Assistant by combining semantic search (FAISS) with Groq's powerful LLaMA-3 model. This system can be scaled with larger snippet databases and integrated into IDEs, chatbots, or dev tools.

---

## ğŸ“ Tech Stack

* Python, FAISS, Sentence Transformers, Groq API, LLaMA-3

---

## ğŸ“ Future Enhancements

* Integrate user feedback for adaptive helpfulness scoring
* Expand codebase with real-world GitHub snippets
* UI integration (e.g., Streamlit or Gradio)

---

Ready to deploy or extend!
